目录
[TOC] 

## 前提
### 训练数据结构
其中第一列`序号`字段是不参与计算的；

| 序号  | 有房者 | 婚姻状况 | 年收入/万 | 拖欠贷款 |
| --- | --- | --- | --- | --- |
| 1 | 是 | 单身 | 125 | 否 |
| 2 | 否 | 已婚 | 100 | 否 |
| 3 | 否 | 单身 | 70 | 否 |
| 4 | 是 | 已婚 | 120 | 否 |
| 5 | 否 | 离异 | 95 | 是 |
| 6 | 否 | 已婚 | 60 | 否 |
| 7 | 是 | 离异 | 220 | 否 |
| 8 | 否 | 单身 | 85 | 是 |
| 9 | 否 | 已婚 | 75 | 否 |
| 10 | 否 | 单身 | 90 | 是 |

### 生成的树
```
婚姻状况
	单身->有房者
		是->否
		否->年收入
			<=85->是
			>85->是
	已婚->否
	离异->有房者
		否->是
		是->否
```
或转为XML文件格式为：
```
<?xml version="1.0" encoding="UTF-8"?>

<root>
  <DecisionTree>
    <婚姻状况 value="单身">
      <有房者 value="是">否</有房者>
      <有房者 value="否">
        <年收入 value="&lt;=85">是</年收入>
        <年收入 value="&gt;85">是</年收入>
      </有房者>
    </婚姻状况>
    <婚姻状况 value="已婚">否</婚姻状况>
    <婚姻状况 value="离异">
      <有房者 value="否">是</有房者>
      <有房者 value="是">否</有房者>
    </婚姻状况>
  </DecisionTree>
</root>
```
## 流程
### 数据加载
- 加载指定文件的数据集`D`；
- 存储到`List<List<String>>`结构中，也就是将每行数据(含字段行)存放到`List`集合`D`中；

### 特征值处理

#### 属性值缺失
本数据集未出现，暂不考虑；
#### 连续变量转换为离散变量
将原始数据集`D`中的连续变量为离散变量；(如：年收入)
- 获得`D`中特征属性值为**连续变量**的列索引集合`colIndexList`（数据集合中特征字段的下标位置）
- 遍历`colIndexList`，获取`D`中某列的全部的连续变量值(<属性-类值>二元组集合，eg:[125, 否]); 将连续变量根据属性值升序排列得到集合`continuouslyVariableList`；计算该特征下的最优分割值`thresholdAttribute`；
- 利用`thresholdAttribute`对该特征进行分割；(如分为“>”和"<="该阈值的两部分)

> ###### 求连续变量的最优分割值：
> 也就是求分割过程中的**最大信息熵**的过程；
> - 遍历`continuouslyVariableList`连续变量集合中的每个属性；
> - 计算利用每个属性把`continuouslyVariableList`集合二分化对应的信息熵`Entropy`；
> - 求二分后的最大信息熵，当前属性就作为最佳分割属性值；
> 
> > ###### 计算利用当前属性二分分割的信息熵
> > - 获取该特征对应的所有类值集合`classifySet`;
> > - 将升序后连续变量集合`continuouslyVariableList`从左到右不断移动(左边从0开始到当前属性下标的区域；右边从当前属性后的值(下标加1)到`continuouslyVariableList`长度减1的区域；)分割为两个部分，分别统计类值`classify`的结果分布，也就是出现次数；得到`leftClassifyCountList`和`rightClassifyCountList`两个集合;
> > - 分布计算左右两部分的信息熵；
> > - 计算利用当前属性进行二分的信息熵；（即左右信息熵占比之和，`(leftCount / totalCount) * infoEntropy(leftClassifyCountList) + (rightCount / totalCount) * infoEntropy(rightClassifyCountList)`）
> > > 熵的公式：
> > > ```math 
> > > H(x) = -\Sigma p_i \log p_i 
> > > ```
> > >  其中： 
> > > ```math 
> > > \boxed{\ p_i =\frac i n}
> > > ```

### 决策树生成
- 输入加载并特征处理后的训练集`D`;
- 信息增益比最大化，得到最优特征`optimalFeatureName`和对应的最大信息增益比`maxIG`；
- 构造树结构`tree`，将`optimalFeatureName`作为树的结点；
- 输入`D`和`tree`去递归生成树的各结点、有向边和叶子结点；
- 最终，生成树模型；

> ##### 信息增益比
> 即**信息增益** 与 **训练数据集D关于特征A的值的熵** 之比；
> - 获取`D`中所有的特征名`featureNameList`，过滤序号和类值字段；
> - 遍历`featureNameList`,计算每个特征的信息增益比；
> - 获得最大信息增益比，得到`maxIGRFeatureName`和`maxIGRatio`；
>
>> 当前特征信息增益比计算：
>>> Gr(D,A) = G(D,A) / Ha(D); 
>>> 其中信息增益为 G(D|A) = H(D) - H(D|A)
>> - 计算当前特征的信息增益`IG`(参考下面的信息增益计算)
>> - 计算训练数据集D关于当前特征的属性值的熵`conditionalEntropy`;(步骤参考信息增益中的条件熵计算)
>> - 得到当前信息增益比`currentIGRatio`（计算当前特征的信息增益**除以**训练数据集关于特征A对区间值（属性）的熵）
>>
>>
>>> **当前特征信息增益计算：**
>>> - 计算当前状态下的总的信息熵`entropy`，即H(D)数据集合D的经验熵;
>>>> - 集合D中类的结果分布：在集合D下的每个类对应的出现次数；
>>>> - 根据信息熵的公式计算，得到当前`D`的信息熵`entropy`；
>>> - 计算当前状态下某一个属性的信息熵H(D|A)，（在特征A给定的条件下D的经验条件熵）。
>>>> - 获得目标特征在原数据中所处的列索引`featureIndex`；
    >>>> - 获取某特征下各个特征值所对应的类值分布`attributeClassifyMap`，如`有房={No=2, Yes=3}`；
>>>> - 遍历`attributeClassifyMap`中的类信息；
>>>> - 计算当前特征下某区间值的信息熵`attributeEntropy`；
>>>> - 输入`D`的样本数目和某属性所对应的类值分布总数，计算当前特征下某区间值的条件概率`attributeProbability`（两者之比）;
>>>> - 计算经验条件熵H(D|A)的值`conditionalEntropy`;
>>>> - 该特征的属性遍历结束后，得到最终的该特征的经验条件熵`conditionalEntropy`;（在该特征给定的条件下D的经验条件熵）
>>>>

>> ##### 树生成：
>> - 获取某特征下的所有属性值`attributeList`；
>> - 获得特征在原数据中所处的列索引；
>> - 遍历`attributeList`该(最优)特征下的属性值`attribute`；
>> - 切分数据集D中的数据：针对每个样本数据，过滤当前(最优)特征及其属性数据（即当前特征的全列数据），得到未参与过计算的其他特征维度数据集合`notUsedFeatureAndValueList`；
>> - 输入`notUsedFeatureAndValueList`和当前最优特征的属性`attribute`和`tree`去构建决策树，也就是`buildDecisionTree`操作；
>> - buildDecisionTree操作中，将`notUsedFeatureAndValueList`当作新的训练集`Di`，计算信息增益比，获得新的最优特征，将新特征继续构建到树中，继续迭代genTreeNode的过程，直到新的信息增益比为0，停止当前轮次迭代，存储叶结点数据。到目前一轮迭代结束，树的某特征下某属性的分支结点和叶子结点就构造完成。
>> - 反复迭代后，完成对每个特征及其属性的计算构建，最终的决策树模型就构造完成了。

### 剪枝
此处的剪枝是根据具体的业务场景来定义的，
针对Tips时：
- 某父结点的子结点的属性值全部相同，直接归为叶结点;
- 父结点下无符合子结点属性的情况下，返回所有子结点的叶子结点数据(业务要求只要满足一个结点就返回所有子结点的类数据);

**效果dmeo：**
```
<?xml version="1.0" encoding="UTF-8"?>

<root>
  <DecisionTree>
    <locations value="SYX">
      <airCompanies value="AC">5b8cd85db0801876ef7ec63b</airCompanies>
      <airCompanies value="GS">5bc45689b080186c8623e564</airCompanies>
      <airCompanies value="MU,HA,JD,OZ">5bc434dd4f3d78d1f73d8aa5</airCompanies>
      <airCompanies value="Negative">5b8cd85db0801876ef7ec63b,5bc45689b080186c8623e564,5bc434dd4f3d78d1f73d8aa5</airCompanies>
    </locations>
    <locations value="EWR">
      <airCompanies value="GS">5bc45689b080186c8623e564</airCompanies>
      <airCompanies value="MU,HA,AC,JD,OZ">5be3b32338b7e628bf7e654b</airCompanies>
      <airCompanies value="Negative">5bc45689b080186c8623e564,5be3b32338b7e628bf7e654b</airCompanies>
    </locations>
    <locations value="AU">5bd6e2c8b0801871bd95a6ca</locations>
    <locations value="DWC,DXB">5b8de9054f3d78d1f73c4208</locations>
    <locations value="SZX,FUO,ZHA,HUZ,MXZ">5b7bc8344f3d78d1f73c2d4c</locations>
  </DecisionTree>
</root>

```
